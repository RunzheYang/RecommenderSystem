\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{1}{subsection.1.1}}
\newlabel{sec:bg}{{1.1}{1}{Background}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Collaborative Filtering}{2}{subsection.1.2}}
\newlabel{sec:cf}{{1.2}{2}{Collaborative Filtering}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{2}{section.2}}
\newlabel{sec:method}{{2}{2}{Methodology}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Item-Based Denoising Autoencoder}{2}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces This illustration is borrowed from \cite  {}. Feed Forward/Backward process for sparse Autoencoders. The sparse input is drawn from the matrix of ratings, unknown values are turned to zero, some ratings are masked (input corruption) and a dense estimate is finally obtained. Before backpropagation, unknown ratings are turned to zero error, prediction errors are reweighed by $\aleph $ and reconstruction errors are reweighed by $\beta $ .}}{2}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Factorization Machine}{3}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces This illustration is b orrowed from []. Example (from Rendle [2010]) for representing a recommender problem with real valued feature vectors x. Every row represents a feature vector x i with its corresponding target y i . For easier interpreta-tion, the features are grouped into indicators for the active user (blue), active item (red), other movies rated by the same user (orange), the time in months (green), and the last movie rated (brown).}}{3}{figure.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments and Results}{4}{section.3}}
\newlabel{sec:exps}{{3}{4}{Experiments and Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data}{4}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces x-axis: movie id; y-axis: the number of watchers. It is a long-tail distribution.}}{4}{figure.3}}
\citation{salakhutdinov2007restricted}
\citation{wang2015collaborative}
\citation{strub2016hybrid}
\citation{dong2017hybrid}
\citation{rendle2012factorization}
\citation{sedhain2015autorec}
\bibstyle{abbrv}
\bibdata{report}
\bibcite{dong2017hybrid}{1}
\bibcite{rendle2012factorization}{2}
\bibcite{salakhutdinov2007restricted}{3}
\bibcite{sedhain2015autorec}{4}
\bibcite{strub2016hybrid}{5}
\bibcite{wang2015collaborative}{6}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Results}{5}{subsection.3.2}}
